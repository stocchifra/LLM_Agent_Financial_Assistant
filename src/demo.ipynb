{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è DATA INSPECTION AND ANALYSIS\n",
    "\n",
    "Before evaluating the models, a detailed inspection of the dataset was conducted to understand the **typology of data** and tailor the evaluation metrics accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Types Distribution\n",
    "\n",
    "The dataset consists of structured QA entries, each containing one or more **expected answers** (`exe_ans`). These answers fall into two primary categories:\n",
    "\n",
    "- **Numerical answers** (e.g., revenue figures, ratios, growth rates)  \n",
    "- **Boolean string answers**, strictly `\"yes\"` or `\"no\"`  \n",
    "\n",
    "Through the analysis of the dataset fields (`qa`, `qa_0`, `qa_1`), it was observed that:\n",
    "\n",
    "- The **vast majority** of entries contain **numerical answers**\n",
    "- Only a **small fraction** of entries involve **string responses**\n",
    "- Among string values, **100%** of them are either `\"yes\"` or `\"no\"` (case-insensitive, whitespace-normalized)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "data_path = \"/Users/francescostocchi/ConvFinQA_LLM_Project/data/train.json\"\n",
    "# directory path\n",
    "directory = \"/Users/francescostocchi/ConvFinQA_LLM_Project/results\"\n",
    "\n",
    "# extract data\n",
    "import json\n",
    "import pandas as pd\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print the first few rows\n",
    "print(df.head())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns\n",
    "df = df.drop(columns=[\"annotation\", \"filename\"])\n",
    "\n",
    "# print a list of df columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of valid (non-null) entries\n",
    "qa = df[\"qa\"].notnull().sum()\n",
    "qa_0 = df[\"qa_0\"].notnull().sum()\n",
    "qa_1 = df[\"qa_1\"].notnull().sum()\n",
    "\n",
    "# Sum them to get the total number of questions\n",
    "total_number_of_questions = qa + qa_0 + qa_1\n",
    "\n",
    "# Print the result\n",
    "print(f\"The total number of questions in the dataset is: {total_number_of_questions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the questions have an exact answer\n",
    "def check_exe_ans(qa):\n",
    "    return isinstance(qa, dict) and \"exe_ans\" in qa\n",
    "\n",
    "# Drop NaNs before applying the check\n",
    "if df[\"qa\"].dropna().apply(check_exe_ans).all() and df[\"qa_0\"].dropna().apply(check_exe_ans).all() and df[\"qa_1\"].dropna().apply(check_exe_ans).all():\n",
    "    print(\"Exact answer is present in all the question fields (excluding NaNs)\")\n",
    "else:\n",
    "    print(\"Some questions are missing the exact answer field (excluding NaNs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the kind of answers \n",
    "def get_exe_ans_type(qa):\n",
    "    if isinstance(qa, dict) and \"exe_ans\" in qa:\n",
    "        val = qa[\"exe_ans\"]\n",
    "        if isinstance(val, (int, float)):\n",
    "            return \"number\"\n",
    "        elif isinstance(val, str):\n",
    "            return \"string\"\n",
    "        else:\n",
    "            return type(val).__name__  # catch other types like list, None, etc.\n",
    "    return \"missing\"\n",
    "\n",
    "# apply the function to the qa column\n",
    "df[\"exe_ans_type_qa\"] = df[\"qa\"].dropna().apply(get_exe_ans_type)\n",
    "\n",
    "print(df[\"exe_ans_type_qa\"].value_counts())\n",
    "\n",
    " # apply the function to the qa_0 column\n",
    "df[\"exe_ans_type_qa_0\"] = df[\"qa_0\"].dropna().apply(get_exe_ans_type)\n",
    "print(df[\"exe_ans_type_qa_0\"].value_counts())\n",
    "\n",
    "# apply the function to the qa_1 column\n",
    "df[\"exe_ans_type_qa_1\"] = df[\"qa_1\"].dropna().apply(get_exe_ans_type)\n",
    "print(df[\"exe_ans_type_qa_1\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the custom palette\n",
    "metavoice_palette = {\n",
    "    'qa': '#309227',   # Green\n",
    "    'qa_0': '#051827', # Dark Blue\n",
    "    'qa_1': '#d8ff02'  # Yellow\n",
    "}\n",
    "\n",
    "\n",
    "# Apply type detection to all three columns without dropna (your function handles missing)\n",
    "df[\"exe_ans_type_qa\"] = df[\"qa\"].apply(get_exe_ans_type)\n",
    "df[\"exe_ans_type_qa_0\"] = df[\"qa_0\"].apply(get_exe_ans_type)\n",
    "df[\"exe_ans_type_qa_1\"] = df[\"qa_1\"].apply(get_exe_ans_type)\n",
    "\n",
    "# Combine results into a single DataFrame for plotting\n",
    "type_data = pd.concat([\n",
    "    df[[\"exe_ans_type_qa\"]].rename(columns={\"exe_ans_type_qa\": \"type\"}).assign(source=\"qa\"),\n",
    "    df[[\"exe_ans_type_qa_0\"]].rename(columns={\"exe_ans_type_qa_0\": \"type\"}).assign(source=\"qa_0\"),\n",
    "    df[[\"exe_ans_type_qa_1\"]].rename(columns={\"exe_ans_type_qa_1\": \"type\"}).assign(source=\"qa_1\"),\n",
    "])\n",
    "\n",
    "# Filter out \"missing\"\n",
    "type_data = type_data[type_data[\"type\"] != \"missing\"]\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=type_data, x=\"type\", hue=\"source\", palette=metavoice_palette)\n",
    "\n",
    "# Bigger title and labels\n",
    "plt.title(\"Distribution of exact answers types across QA sources\", fontsize=18, weight='bold')\n",
    "plt.xlabel(\"Exact answers type\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "\n",
    "# Increase tick label sizes\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Legend title and text size\n",
    "plt.legend(title=\"QA Source\", title_fontsize=13, fontsize=12)\n",
    "\n",
    "# Optional: log scale\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ALL string values in exe_ans are either \"yes\" or \"no\"\n",
    "def all_string_answers_yes_no(column):\n",
    "    return not column.apply(\n",
    "        lambda qa: isinstance(qa, dict) and isinstance(qa.get(\"exe_ans\"), str) and qa[\"exe_ans\"].strip().lower() not in [\"yes\", \"no\"]\n",
    "    ).any()\n",
    "\n",
    "if all_string_answers_yes_no(df[\"qa\"]) and all_string_answers_yes_no(df[\"qa_0\"]) and all_string_answers_yes_no(df[\"qa_1\"]):\n",
    "    print(\"Yes ‚Äî all string answers are either 'yes' or 'no'\")\n",
    "else:\n",
    "    print(\"Some string answers are NOT 'yes' or 'no'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† AGENTIC MODEL EVALUATION\n",
    "\n",
    "This experiment evaluates the performance of three leading LLMs:\n",
    "- **GPT-4o** from OpenAI  \n",
    "- **Claude 3.7 Sonnet** from Anthropic  \n",
    "- **Gemini 2.0 Flash** from Google  \n",
    "\n",
    "### Prompting Strategies (Prompt Styles)\n",
    "\n",
    "The evaluation includes three distinct **prompt styles**, each designed to test different agent capabilities:\n",
    "\n",
    "| Prompt Style   | Name          | Description |\n",
    "|----------------|---------------|-------------|\n",
    "| **JSON-Chat**  | `json-chat`   | A custom prompt template using a **system** and **user prompt** defined in a structured way within the prompt templates file. This prompt is optimized for clarity and consistency in numerical QA. |\n",
    "| **ReAct**      | `react`       | A **ReAct-style** prompt where the model is guided to follow a reasoning-then-acting structure. It mimics classical step-by-step CoT (Chain-of-Thought) reasoning followed by action. |\n",
    "| **Tool Agent** | `tools-agent` | A tool-oriented prompt that explicitly encourages the model to use **tool calls**. It's crafted to test how well the model can interact with external tools (e.g., calculators, parsers) during reasoning. |\n",
    "\n",
    "Each prompt style is evaluated independently to observe how **prompt engineering** affects model performance on both numeric and boolean tasks.\n",
    "\n",
    "---\n",
    "\n",
    "These prompt strategies help assess:\n",
    "- The model‚Äôs ability to **reason step-by-step**\n",
    "- Its capacity to **trigger and use external tools**\n",
    "- Its consistency in producing **well-formatted, verifiable answers**\n",
    "The evaluation focuses on **numerical accuracy** and **string classification correctness** over a randomly selected sample of **20 entries**, which represents approximately **1% of the total dataset**. Due to resource constraints (API call limits), this sample offers only a **low-statistical-validity** glimpse into the models' capabilities. However, the methodology scales to the full dataset for comprehensive evaluation when needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Methodology\n",
    "\n",
    "The evaluation is conducted using a custom metric function: `measure_accuracy`, supported by two helper functions: `compute_single_sample_accuracy` and `evaluate_answer`. Here's an explanation of each.\n",
    "\n",
    "---\n",
    "\n",
    "#### `measure_accuracy(...)`\n",
    "\n",
    "This is the **main evaluation function**, which:\n",
    "- Randomly samples a subset of questions from a given dataset.\n",
    "- Prompts the agentic model to answer the questions.\n",
    "- Compares the model‚Äôs output (`actual_answers`) with the ground truth (`expected_answers`).\n",
    "- Returns key metrics:  \n",
    "  - `mean_accuracy`: overall % of correct answers  \n",
    "  - `mae`: mean absolute error for numeric predictions  \n",
    "  - `mse`: mean squared error for numeric predictions  \n",
    "  - `accuracy_measurements`: list of 0/1 indicating per-answer correctness\n",
    "  - `llm_average_score`: overall % of correct answers \n",
    "\n",
    "A **tolerance margin** is applied to numeric comparisons to account for minor variations. Answers outside the tolerance are considered incorrect. For string answers (e.g., \"yes\"/\"no\"), a strict match is used after lowercasing and removing spaces.\n",
    "\n",
    "---\n",
    "\n",
    "#### `compute_single_sample_accuracy(...)`\n",
    "\n",
    "This sub-function performs **answer-level comparison**:\n",
    "- For **numeric answers**:  \n",
    "  - Compares using `abs(expected - actual)`  \n",
    "  - Applies the `tolerance` threshold to mark correctness (score 1 or 0)  \n",
    "  - Calculates **MAE** and **MSE**\n",
    "- For **string answers**:  \n",
    "  - Performs strict comparison after normalization (lowercased, whitespace removed)  \n",
    "  - Only \"yes\" and \"no\" are valid answers; all others are treated as incorrect\n",
    "- Supports **asymmetric lengths** between predicted and expected answers by scoring missing answers as incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "#### `evaluate_answer(...)`\n",
    "\n",
    "This optional function introduces an **LLM-as-a-judge** approach:\n",
    "- Used when the model's answer might be in an unexpected format (e.g., verbose explanations, embedded numbers, misformatted strings).\n",
    "- It sends the original question, expected answer, and the model's response to another LLM acting as a \"judge\".\n",
    "- The judge returns a score (e.g., 1 or 0) and an **explanation**.\n",
    "\n",
    "This provides an **additional quality-control layer** that can catch cases where a numerically correct answer is returned in the wrong format and would otherwise be unfairly marked as incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics Summary\n",
    "\n",
    "Since most of the answers in the dataset are **numerical**, the metrics naturally emphasize:\n",
    "- **Numerical Accuracy**\n",
    "- **MAE (Mean Absolute Error)**\n",
    "- **MSE (Mean Squared Error)**\n",
    "\n",
    "The few **string-based answers** (all either \"yes\" or \"no\") are treated as **binary classification tasks** using exact matching.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "- **Lightweight**: The method can evaluate a small sample with minimal cost.\n",
    "- **Scalable**: Easily extends to evaluate the full dataset.\n",
    "- **Robust**: Supports both numeric and string answer types.\n",
    "- **Adaptable**: The LLM-judge adds human-like reasoning for edge cases.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- The 20-sample evaluation is **not statistically significant** and meant only for indicative benchmarking.\n",
    "- String answer evaluation is **strict** ‚Äî any extra formatting or words can cause a mismatch unless the LLM-judge is used.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Save the metrics to a JSON file\n",
    "import json \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Go up one directory to reach the project root\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "import src.utils.data_extractor\n",
    "importlib.reload(src.utils.data_extractor)\n",
    "import importlib\n",
    "import src.metrics.accuracy  \n",
    "importlib.reload(src.metrics.accuracy)\n",
    "import src.agent.agent_builder\n",
    "#importlib.reload(src.agent.agent_builder)\n",
    "import src.agent.agent_tools\n",
    "importlib.reload(src.agent.agent_tools)\n",
    "import src.agent.prompt_templates\n",
    "importlib.reload(src.agent.prompt_templates)\n",
    "\n",
    "# Re-import the function\n",
    "from src.metrics.accuracy import measure_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of samples\n",
    "number_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING OPENAI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "provider = \"openai\"\n",
    "tolerance = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"react\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_gpt4o_react = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gpt-4o_react:\")\n",
    "for key, value in metrics_gpt4o_react.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "# Save the metrics to a JSON file\n",
    "import json \n",
    "from datetime import datetime\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gpt4o_react, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING JSON-CHAT PROMPT STYLE\n",
    "prompt_style = \"json-chat\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_gpt4o_custom = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gpt-4o_customt:\")\n",
    "for key, value in metrics_gpt4o_custom.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gpt4o_custom, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING TOOLS-AGENT PROMPT STYLE\n",
    "prompt_style = \"few-shot-CoT\"  \n",
    "\n",
    "# save the the metrics for TOOLS-AGENT \n",
    "metrics_gpt4o_cot = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gpt-4o_tool:\")\n",
    "for key, value in metrics_gpt4o_cot.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gpt4o_cot, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING ANTHROPIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"claude-3-7-sonnet-20250219\"\n",
    "# model = \"claude-3-5-haiku-20241022\"\n",
    "model = \"claude-3-5-sonnet-20241022\"\n",
    "provider = \"anthropic\"\n",
    "tolerance = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"react\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_sonnet3_7_react = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for sonnet_3_5_react:\")\n",
    "for key, value in metrics_sonnet3_7_react.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_sonnet3_7_react, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING JSON-CHAT PROMPT STYLE\n",
    "prompt_style = \"json-chat\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_sonnet3_7_custom = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for sonnet_3_5_custom:\")\n",
    "for key, value in metrics_sonnet3_7_custom.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_sonnet3_7_custom, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING  PROMPT STYLE\n",
    "prompt_style = \"few-shot-CoT\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_sonnet3_7_cot = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for sonnet3_5_few_shot_react:\")\n",
    "for key, value in metrics_sonnet3_7_cot.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_sonnet3_7_cot, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING GOOGLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.0-flash\"\n",
    "provider = \"google\"\n",
    "tolerance = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"react\"  \n",
    "\n",
    "metrics_gemini_react = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gemini_react:\")\n",
    "for key, value in metrics_gemini_react.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gemini_react, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"json-chat\"  \n",
    "\n",
    "metrics_gemini_json_chat = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gemini_custom:\")\n",
    "for key, value in metrics_gemini_json_chat.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gemini_json_chat, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"few-shot-CoT\"  \n",
    "\n",
    "metrics_gemini_cot = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gemini_custom:\")\n",
    "for key, value in metrics_gemini_cot.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gemini_cot, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convfinq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
