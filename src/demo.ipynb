{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è DATA INSPECTION AND ANALYSIS\n",
    "\n",
    "Before evaluating the models, a detailed inspection of the dataset was conducted to understand the **typology of data** and tailor the evaluation metrics accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Types Distribution\n",
    "\n",
    "The dataset consists of structured QA entries, each containing one or more **expected answers** (`exe_ans`). These answers fall into two primary categories:\n",
    "\n",
    "- **Numerical answers** (e.g., revenue figures, ratios, growth rates)  \n",
    "- **Boolean string answers**, strictly `\"yes\"` or `\"no\"`  \n",
    "\n",
    "Through the analysis of the dataset fields (`qa`, `qa_0`, `qa_1`), it was observed that:\n",
    "\n",
    "- The **vast majority** of entries contain **numerical answers**\n",
    "- Only a **small fraction** of entries involve **string responses**\n",
    "- Among string values, **100%** of them are either `\"yes\"` or `\"no\"` (case-insensitive, whitespace-normalized)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "data_path = \":data/train.json\"\n",
    "# directory path\n",
    "directory = \":data/results\"\n",
    "\n",
    "# extract data\n",
    "import json\n",
    "import pandas as pd\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print the first few rows\n",
    "print(df.head())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns\n",
    "df = df.drop(columns=[\"annotation\", \"filename\"])\n",
    "\n",
    "# print a list of df columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of valid (non-null) entries\n",
    "qa = df[\"qa\"].notnull().sum()\n",
    "qa_0 = df[\"qa_0\"].notnull().sum()\n",
    "qa_1 = df[\"qa_1\"].notnull().sum()\n",
    "\n",
    "# Sum them to get the total number of questions\n",
    "total_number_of_questions = qa + qa_0 + qa_1\n",
    "\n",
    "# Print the result\n",
    "print(f\"The total number of questions in the dataset is: {total_number_of_questions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the questions have an exact answer\n",
    "def check_exe_ans(qa):\n",
    "    return isinstance(qa, dict) and \"exe_ans\" in qa\n",
    "\n",
    "# Drop NaNs before applying the check\n",
    "if df[\"qa\"].dropna().apply(check_exe_ans).all() and df[\"qa_0\"].dropna().apply(check_exe_ans).all() and df[\"qa_1\"].dropna().apply(check_exe_ans).all():\n",
    "    print(\"Exact answer is present in all the question fields (excluding NaNs)\")\n",
    "else:\n",
    "    print(\"Some questions are missing the exact answer field (excluding NaNs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the kind of answers \n",
    "def get_exe_ans_type(qa):\n",
    "    if isinstance(qa, dict) and \"exe_ans\" in qa:\n",
    "        val = qa[\"exe_ans\"]\n",
    "        if isinstance(val, (int, float)):\n",
    "            return \"number\"\n",
    "        elif isinstance(val, str):\n",
    "            return \"string\"\n",
    "        else:\n",
    "            return type(val).__name__  # catch other types like list, None, etc.\n",
    "    return \"missing\"\n",
    "\n",
    "# apply the function to the qa column\n",
    "df[\"exe_ans_type_qa\"] = df[\"qa\"].dropna().apply(get_exe_ans_type)\n",
    "\n",
    "print(df[\"exe_ans_type_qa\"].value_counts())\n",
    "\n",
    " # apply the function to the qa_0 column\n",
    "df[\"exe_ans_type_qa_0\"] = df[\"qa_0\"].dropna().apply(get_exe_ans_type)\n",
    "print(df[\"exe_ans_type_qa_0\"].value_counts())\n",
    "\n",
    "# apply the function to the qa_1 column\n",
    "df[\"exe_ans_type_qa_1\"] = df[\"qa_1\"].dropna().apply(get_exe_ans_type)\n",
    "print(df[\"exe_ans_type_qa_1\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seaborn styling and serif font\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Define the custom palette\n",
    "tomoro_palette = {\n",
    "    'qa': '#309227',   # Green\n",
    "    'qa_0': '#051827', # Dark Blue\n",
    "    'qa_1': '#d8ff02'  # Yellow\n",
    "}\n",
    "\n",
    "# Apply type detection to all three columns\n",
    "df[\"exe_ans_type_qa\"] = df[\"qa\"].apply(get_exe_ans_type)\n",
    "df[\"exe_ans_type_qa_0\"] = df[\"qa_0\"].apply(get_exe_ans_type)\n",
    "df[\"exe_ans_type_qa_1\"] = df[\"qa_1\"].apply(get_exe_ans_type)\n",
    "\n",
    "# Combine results into a single DataFrame for plotting\n",
    "type_data = pd.concat([\n",
    "    df[[\"exe_ans_type_qa\"]].rename(columns={\"exe_ans_type_qa\": \"type\"}).assign(source=\"qa\"),\n",
    "    df[[\"exe_ans_type_qa_0\"]].rename(columns={\"exe_ans_type_qa_0\": \"type\"}).assign(source=\"qa_0\"),\n",
    "    df[[\"exe_ans_type_qa_1\"]].rename(columns={\"exe_ans_type_qa_1\": \"type\"}).assign(source=\"qa_1\"),\n",
    "])\n",
    "\n",
    "# Filter out \"missing\"\n",
    "type_data = type_data[type_data[\"type\"] != \"missing\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.countplot(\n",
    "    data=type_data,\n",
    "    x=\"type\",\n",
    "    hue=\"source\",\n",
    "    palette=tomoro_palette,\n",
    "    edgecolor=\"gray\",\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Distribution of Exact Answer Types Across QA Sources\", fontsize=18, weight='bold', pad=15)\n",
    "plt.xlabel(\"Exact Answer Type\", fontsize=14, labelpad=8)\n",
    "plt.ylabel(\"Count (Log Scale)\", fontsize=14, labelpad=8)\n",
    "\n",
    "# Axes styling\n",
    "plt.xticks(fontsize=12, rotation=10)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "sns.despine()\n",
    "\n",
    "# Add clean value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%d\", label_type=\"edge\", fontsize=11, padding=2)\n",
    "\n",
    "# Legend styling\n",
    "legend = plt.legend(\n",
    "    title=\"QA Source\", title_fontsize=13, fontsize=12,\n",
    "    loc='upper right', frameon=False\n",
    ")\n",
    "\n",
    "# Final layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ALL string values in exe_ans are either \"yes\" or \"no\"\n",
    "def all_string_answers_yes_no(column):\n",
    "    return not column.apply(\n",
    "        lambda qa: isinstance(qa, dict) and isinstance(qa.get(\"exe_ans\"), str) and qa[\"exe_ans\"].strip().lower() not in [\"yes\", \"no\"]\n",
    "    ).any()\n",
    "\n",
    "if all_string_answers_yes_no(df[\"qa\"]) and all_string_answers_yes_no(df[\"qa_0\"]) and all_string_answers_yes_no(df[\"qa_1\"]):\n",
    "    print(\"Yes ‚Äî all string answers are either 'yes' or 'no'\")\n",
    "else:\n",
    "    print(\"Some string answers are NOT 'yes' or 'no'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† AGENTIC MODEL EVALUATION\n",
    "\n",
    "This experiment evaluates the performance of three leading LLMs:\n",
    "- **GPT-4o** from OpenAI  \n",
    "- **Claude 3.7 Sonnet** from Anthropic  \n",
    "- **Gemini 2.0 Flash** from Google  \n",
    "\n",
    "### Prompting Strategies (Prompt Styles)\n",
    "\n",
    "The evaluation includes three distinct **prompt styles**, each designed to test different agent capabilities:\n",
    "\n",
    "| Prompt Style   | Name          | Description |\n",
    "|----------------|---------------|-------------|\n",
    "| **JSON-Chat**  | `json-chat`   | A custom prompt template using a **system** and **user prompt** defined in a structured way within the prompt templates file. This prompt is optimized for clarity and consistency in numerical QA. |\n",
    "| **ReAct**      | `react`       | A **ReAct-style** prompt where the model is guided to follow a reasoning-then-acting structure. It mimics classical step-by-step CoT (Chain-of-Thought) reasoning followed by action. |\n",
    "| **Tool Agent** | `tools-agent` | A tool-oriented prompt that explicitly encourages the model to use **tool calls**. It's crafted to test how well the model can interact with external tools (e.g., calculators, parsers) during reasoning. |\n",
    "\n",
    "Each prompt style is evaluated independently to observe how **prompt engineering** affects model performance on both numeric and boolean tasks.\n",
    "\n",
    "---\n",
    "\n",
    "These prompt strategies help assess:\n",
    "- The model‚Äôs ability to **reason step-by-step**\n",
    "- Its capacity to **trigger and use external tools**\n",
    "- Its consistency in producing **well-formatted, verifiable answers**\n",
    "The evaluation focuses on **numerical accuracy** and **string classification correctness** over a randomly selected sample of **20 entries**, which represents approximately **1% of the total dataset**. Due to resource constraints (API call limits), this sample offers only a **low-statistical-validity** glimpse into the models' capabilities. However, the methodology scales to the full dataset for comprehensive evaluation when needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Methodology\n",
    "\n",
    "The evaluation is conducted using a custom metric function: `measure_accuracy`, supported by two helper functions: `compute_single_sample_accuracy` and `evaluate_answer`. Here's an explanation of each.\n",
    "\n",
    "---\n",
    "\n",
    "#### `measure_accuracy(...)`\n",
    "\n",
    "This is the **main evaluation function**, which:\n",
    "- Randomly samples a subset of questions from a given dataset.\n",
    "- Prompts the agentic model to answer the questions.\n",
    "- Compares the model‚Äôs output (`actual_answers`) with the ground truth (`expected_answers`).\n",
    "- Returns key metrics:  \n",
    "  - `mean_accuracy`: overall % of correct answers  \n",
    "  - `mae`: mean absolute error for numeric predictions  \n",
    "  - `mse`: mean squared error for numeric predictions  \n",
    "  - `accuracy_measurements`: list of 0/1 indicating per-answer correctness\n",
    "  - `llm_average_score`: overall % of correct answers \n",
    "\n",
    "A **tolerance margin** is applied to numeric comparisons to account for minor variations. Answers outside the tolerance are considered incorrect. For string answers (e.g., \"yes\"/\"no\"), a strict match is used after lowercasing and removing spaces.\n",
    "\n",
    "---\n",
    "\n",
    "#### `compute_single_sample_accuracy(...)`\n",
    "\n",
    "This sub-function performs **answer-level comparison**:\n",
    "- For **numeric answers**:  \n",
    "  - Compares using `abs(expected - actual)`  \n",
    "  - Applies the `tolerance` threshold to mark correctness (score 1 or 0)  \n",
    "  - Calculates **MAE** and **MSE**\n",
    "- For **string answers**:  \n",
    "  - Performs strict comparison after normalization (lowercased, whitespace removed)  \n",
    "  - Only \"yes\" and \"no\" are valid answers; all others are treated as incorrect\n",
    "- Supports **asymmetric lengths** between predicted and expected answers by scoring missing answers as incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "#### `evaluate_answer(...)`\n",
    "\n",
    "This optional function introduces an **LLM-as-a-judge** approach:\n",
    "- Used when the model's answer might be in an unexpected format (e.g., verbose explanations, embedded numbers, misformatted strings).\n",
    "- It sends the original question, expected answer, and the model's response to another LLM acting as a \"judge\".\n",
    "- The judge returns a score (e.g., 1 or 0) and an **explanation**.\n",
    "\n",
    "This provides an **additional quality-control layer** that can catch cases where a numerically correct answer is returned in the wrong format and would otherwise be unfairly marked as incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics Summary\n",
    "\n",
    "Since most of the answers in the dataset are **numerical**, the metrics naturally emphasize:\n",
    "- **Numerical Accuracy**\n",
    "- **MAE (Mean Absolute Error)**\n",
    "- **MSE (Mean Squared Error)**\n",
    "\n",
    "The few **string-based answers** (all either \"yes\" or \"no\") are treated as **binary classification tasks** using exact matching.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "- **Lightweight**: The method can evaluate a small sample with minimal cost.\n",
    "- **Scalable**: Easily extends to evaluate the full dataset.\n",
    "- **Robust**: Supports both numeric and string answer types.\n",
    "- **Adaptable**: The LLM-judge adds human-like reasoning for edge cases.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- The 20-sample evaluation is **not statistically significant** and meant only for indicative benchmarking.\n",
    "- String answer evaluation is **strict** ‚Äî any extra formatting or words can cause a mismatch unless the LLM-judge is used.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Save the metrics to a JSON file\n",
    "import json \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Go up one directory to reach the project root\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "import src.utils.data_extractor\n",
    "importlib.reload(src.utils.data_extractor)\n",
    "import importlib\n",
    "import src.metrics.accuracy  \n",
    "importlib.reload(src.metrics.accuracy)\n",
    "import src.agent.agent_builder\n",
    "#importlib.reload(src.agent.agent_builder)\n",
    "import src.agent.agent_tools\n",
    "importlib.reload(src.agent.agent_tools)\n",
    "import src.agent.prompt_templates\n",
    "importlib.reload(src.agent.prompt_templates)\n",
    "\n",
    "# Re-import the function\n",
    "from src.metrics.accuracy import measure_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of samples\n",
    "number_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING OPENAI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "provider = \"openai\"\n",
    "tolerance = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"react\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_gpt4o_react = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gpt-4o_react:\")\n",
    "for key, value in metrics_gpt4o_react.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "# Save the metrics to a JSON file\n",
    "import json \n",
    "from datetime import datetime\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gpt4o_react, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING JSON-CHAT PROMPT STYLE\n",
    "prompt_style = \"json-chat\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_gpt4o_custom = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gpt-4o_customt:\")\n",
    "for key, value in metrics_gpt4o_custom.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gpt4o_custom, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING TOOLS-AGENT PROMPT STYLE\n",
    "prompt_style = \"few-shot-CoT\"  \n",
    "\n",
    "# save the the metrics for TOOLS-AGENT \n",
    "metrics_gpt4o_cot = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gpt-4o_tool:\")\n",
    "for key, value in metrics_gpt4o_cot.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gpt4o_cot, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING ANTHROPIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"claude-3-7-sonnet-20250219\"\n",
    "# model = \"claude-3-5-haiku-20241022\"\n",
    "model = \"claude-3-5-sonnet-20241022\"\n",
    "provider = \"anthropic\"\n",
    "tolerance = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"react\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_sonnet3_7_react = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for sonnet_3_5_react:\")\n",
    "for key, value in metrics_sonnet3_7_react.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_sonnet3_7_react, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING JSON-CHAT PROMPT STYLE\n",
    "prompt_style = \"json-chat\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_sonnet3_7_custom = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for sonnet_3_5_custom:\")\n",
    "for key, value in metrics_sonnet3_7_custom.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_sonnet3_7_custom, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING  PROMPT STYLE\n",
    "prompt_style = \"few-shot-CoT\"  \n",
    "\n",
    "# save the the metrics for gpt-4o_react \n",
    "metrics_sonnet3_7_cot = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for sonnet3_5_few_shot_react:\")\n",
    "for key, value in metrics_sonnet3_7_cot.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_sonnet3_7_cot, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING GOOGLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemini-2.0-flash\"\n",
    "provider = \"google\"\n",
    "tolerance = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"react\"  \n",
    "\n",
    "metrics_gemini_react = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gemini_react:\")\n",
    "for key, value in metrics_gemini_react.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gemini_react, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"json-chat\"  \n",
    "\n",
    "metrics_gemini_json_chat = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gemini_custom:\")\n",
    "for key, value in metrics_gemini_json_chat.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gemini_json_chat, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING REACT PROMPT STYLE\n",
    "prompt_style = \"few-shot-CoT\"  \n",
    "\n",
    "metrics_gemini_cot = measure_accuracy(\n",
    "    data_path=data_path,\n",
    "    model=model,\n",
    "    provider=provider,\n",
    "    prompt_style=prompt_style,\n",
    "    tolerance=tolerance,\n",
    "    number_samples=number_samples,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the metrics\n",
    "print(\"Metrics for gemini_custom:\")\n",
    "for key, value in metrics_gemini_cot.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create a timestamp for uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define directory and dynamic filename\n",
    "filename = f\"metrics_{model}_{prompt_style}_{timestamp}.json\"\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the metrics to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(metrics_gemini_cot, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTS FOR COMPARISON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path to the results folder\n",
    "results_folder = \"/Users/francescostocchi/ConvFinQA_LLM_Project/results\"\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "# Helper function to normalize names\n",
    "def normalize_model_prompt(filename):\n",
    "    name = filename.lower()\n",
    "    \n",
    "    # Determine model name\n",
    "    if \"gpt-4o\" in name:\n",
    "        model = \"gpt-4o\"\n",
    "    elif \"claude-3-5-sonnet\" in name:\n",
    "        model = \"claude-3.5-sonnet\"\n",
    "    elif \"claude-3-7-sonnet\" in name:\n",
    "        model = \"claude-3.7-sonnet\"\n",
    "    elif \"gemini\" in name:\n",
    "        model = \"gemini-2.0-flash\"\n",
    "    else:\n",
    "        model = \"unknown\"\n",
    "\n",
    "    # Determine prompt type\n",
    "    if \"few-shot-cot\" in name:\n",
    "        prompt = \"few-shot-CoT\"\n",
    "    elif \"json-chat\" in name:\n",
    "        prompt = \"json-chat\"\n",
    "    elif \"react\" in name:\n",
    "        prompt = \"react\"\n",
    "    else:\n",
    "        prompt = \"unknown\"\n",
    "\n",
    "    return model, prompt\n",
    "\n",
    "# Iterate through files in the results folder\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(results_folder, filename)\n",
    "        \n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        model, prompt = normalize_model_prompt(filename)\n",
    "        if model not in model_results:\n",
    "            model_results[model] = {}\n",
    "\n",
    "        # Save relevant metrics\n",
    "        model_results[model][prompt] = {\n",
    "            \"mean_accuracy\": data.get(\"mean_accuracy\"),\n",
    "            \"mae\": data.get(\"mae\"),\n",
    "            \"mse\": data.get(\"mse\"),\n",
    "            \"llm_average_score\": data.get(\"llm_average_score\"),\n",
    "        }\n",
    "\n",
    "# print the results\n",
    "from pprint import pprint\n",
    "pprint(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARISON OF MODELS FOR EACH PROMPT\n",
    "\n",
    "# custom palette for plotting\n",
    "tomoro_palette = {\n",
    "    'gpt-4o': '#309227',   # Green\n",
    "    'claude-3.5-sonnet': '#051827', # Dark Blue\n",
    "    'gemini-2.0-flash': '#d8ff02'  # Yellow\n",
    "}\n",
    "\n",
    "# Prepare the DataFrame\n",
    "rows = []\n",
    "for model, prompts in model_results.items():\n",
    "    for prompt, metrics in prompts.items():\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"mean_accuracy\": metrics.get(\"mean_accuracy\", 0)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Create one plot per prompt\n",
    "for prompt_name in df[\"prompt\"].unique():\n",
    "    prompt_df = df[df[\"prompt\"] == prompt_name]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=prompt_df, x=\"model\", y=\"mean_accuracy\", palette=tomoro_palette)\n",
    "\n",
    "    # Title and labels\n",
    "    plt.title(f\"Model Comparison for Prompt: {prompt_name}\", fontsize=18, weight='bold')\n",
    "    plt.xlabel(\"Model\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Accuracy\", fontsize=14)\n",
    "\n",
    "    # Styling\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=15, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Your current palette:\n",
    "tomoro_palette = {\n",
    "    'json-chat': '#309227',     # Green\n",
    "    'few-shot-CoT': '#051827',  # Dark Blue\n",
    "    'react': '#d8ff02'          # Yellow\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model, prompts in model_results.items():\n",
    "    # Build DataFrame\n",
    "    data = {\n",
    "        \"prompt\": list(prompts.keys()),\n",
    "        \"mean_accuracy\": [metrics[\"mean_accuracy\"] for metrics in prompts.values()]\n",
    "    }\n",
    "    df_model = pd.DataFrame(data)\n",
    "\n",
    "    # Match color order from tomoro_palette\n",
    "    colors = [tomoro_palette[p] for p in df_model[\"prompt\"]]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(\n",
    "        data=df_model,\n",
    "        x=\"prompt\",\n",
    "        y=\"mean_accuracy\",\n",
    "        palette=colors,\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    # Titles and labels\n",
    "    plt.title(f\"Prompt Comparison for {model}\", fontsize=20, weight='bold', pad=20)\n",
    "    plt.xlabel(\"Prompt Type\", fontsize=16, labelpad=10)\n",
    "    plt.ylabel(\"Mean Accuracy\", fontsize=16, labelpad=10)\n",
    "\n",
    "    # Ticks\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Grid\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", fontsize=12, padding=3)\n",
    "\n",
    "    # Final layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARE ACROSS ALL PROMPTS AND MODELS\n",
    "# Your current palette:\n",
    "tomoro_palette = {\n",
    "    'gpt-4o': '#309227',     # Green\n",
    "    'claude-3.5-sonnet': '#051827',  # Dark Blue\n",
    "    'gemini-2.0-flash': '#d8ff02'          # Yellow\n",
    "}\n",
    "\n",
    "# Seaborn styling\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Font style (optional but more academic)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Normalize prompt labels (in case needed for other logic)\n",
    "def normalize_prompt(p):\n",
    "    return p.lower().replace(\" \", \"-\")\n",
    "\n",
    "df[\"prompt_normalized\"] = df[\"prompt\"].apply(normalize_prompt)\n",
    "\n",
    "# Sort for consistent groupings\n",
    "df = df.sort_values(by=[\"prompt\", \"model\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"prompt\",\n",
    "    y=\"mean_accuracy\",\n",
    "    hue=\"model\",\n",
    "    palette=tomoro_palette,\n",
    "    edgecolor=\"gray\",  # subtle bar edge\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Mean Accuracy Comparison Across Models and Prompts\", fontsize=18, weight='bold', pad=15)\n",
    "plt.xlabel(\"Prompt Type\", fontsize=14, labelpad=8)\n",
    "plt.ylabel(\"Mean Accuracy\", fontsize=14, labelpad=8)\n",
    "\n",
    "# Axes styling\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "sns.despine()  # Removes top and right border\n",
    "\n",
    "# Add clean value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", fontsize=11, padding=2)\n",
    "\n",
    "# Legend styling\n",
    "legend = plt.legend(\n",
    "    title=\"Model\", title_fontsize=13, fontsize=12,\n",
    "    loc='upper left', frameon=False\n",
    ")\n",
    "\n",
    "# Final layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARE PROMPTS ACROSS ALL MODELS\n",
    "\n",
    "# Set seaborn styling\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Your current palette:\n",
    "tomoro_palette = {\n",
    "    'json-chat': '#309227',     # Green\n",
    "    'few-shot-CoT': '#051827',  # Dark Blue\n",
    "    'react': '#d8ff02'          # Yellow\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model, prompts in model_results.items():\n",
    "    # Build DataFrame\n",
    "    data = {\n",
    "        \"prompt\": list(prompts.keys()),\n",
    "        \"llm_average_score\": [metrics[\"llm_average_score\"] for metrics in prompts.values()]\n",
    "    }\n",
    "    df_model = pd.DataFrame(data)\n",
    "\n",
    "    # Match color order from tomoro_palette\n",
    "    colors = [tomoro_palette[p] for p in df_model[\"prompt\"]]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(\n",
    "        data=df_model,\n",
    "        x=\"prompt\",\n",
    "        y=\"llm_average_score\",\n",
    "        palette=colors,\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    # Titles and labels\n",
    "    plt.title(f\"Prompt Comparison for {model}\", fontsize=20, weight='bold', pad=20)\n",
    "    plt.xlabel(\"Prompt Type\", fontsize=16, labelpad=10)\n",
    "    plt.ylabel(\"LLM Average Score\", fontsize=16, labelpad=10)\n",
    "\n",
    "    # Ticks\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Grid\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", fontsize=12, padding=3)\n",
    "\n",
    "    # Final layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARE ACROSS ALL PROMPTS AND MODELS USING LLM AVERAGE SCORE\n",
    "## COMPARE ACROSS ALL PROMPTS AND MODELS\n",
    "# Your current palette:\n",
    "tomoro_palette = {\n",
    "    'gpt-4o': '#309227',     # Green\n",
    "    'claude-3.5-sonnet': '#051827',  # Dark Blue\n",
    "    'gemini-2.0-flash': '#d8ff02'          # Yellow\n",
    "}\n",
    "# Rebuild df from model_results with llm_average_score\n",
    "rows = []\n",
    "for model, prompts in model_results.items():\n",
    "    for prompt, metrics in prompts.items():\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"llm_average_score\": metrics.get(\"llm_average_score\", None)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Seaborn styling\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Font style (optional but more academic)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Normalize prompt labels (in case needed for other logic)\n",
    "def normalize_prompt(p):\n",
    "    return p.lower().replace(\" \", \"-\")\n",
    "\n",
    "df[\"prompt_normalized\"] = df[\"prompt\"].apply(normalize_prompt)\n",
    "\n",
    "# Sort for consistent groupings\n",
    "df = df.sort_values(by=[\"prompt\", \"model\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"prompt\",\n",
    "    y=\"llm_average_score\",\n",
    "    hue=\"model\",\n",
    "    palette=tomoro_palette,\n",
    "    edgecolor=\"gray\",  # subtle bar edge\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"LLM Average Score Comparison Across Models and Prompts\", fontsize=18, weight='bold', pad=15)\n",
    "plt.xlabel(\"Prompt Type\", fontsize=14, labelpad=8)\n",
    "plt.ylabel(\"LLM Average Score\", fontsize=14, labelpad=8)\n",
    "\n",
    "# Axes styling\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "sns.despine()  # Removes top and right border\n",
    "\n",
    "# Add clean value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", fontsize=11, padding=2)\n",
    "\n",
    "# Legend styling\n",
    "legend = plt.legend(\n",
    "    title=\"Model\", title_fontsize=13, fontsize=12,\n",
    "    loc='upper left', frameon=False\n",
    ")\n",
    "\n",
    "# Final layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convfinq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
